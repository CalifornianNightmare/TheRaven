{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T19:40:26.432014Z",
     "start_time": "2024-11-04T19:40:13.361514Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.23.5)\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.1)\r\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (1.23.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2023.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.4.0)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.23.5)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.12.0)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.2.2)\r\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.10.0)\r\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2024.9.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T12:56:02.603551Z",
     "start_time": "2024-11-07T12:56:02.565894Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import roc_auc_score,precision_score,recall_score,confusion_matrix, accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T12:57:07.016533Z",
     "start_time": "2024-11-07T12:57:03.839516Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_409</th>\n",
       "      <th>feature_410</th>\n",
       "      <th>feature_411</th>\n",
       "      <th>feature_412</th>\n",
       "      <th>feature_413</th>\n",
       "      <th>feature_414</th>\n",
       "      <th>feature_415</th>\n",
       "      <th>feature_416</th>\n",
       "      <th>feature_417</th>\n",
       "      <th>feature_418</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.468142</td>\n",
       "      <td>-1.045346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.384487</td>\n",
       "      <td>0.435121</td>\n",
       "      <td>-1.178548</td>\n",
       "      <td>0.124543</td>\n",
       "      <td>1.801544</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.361507</td>\n",
       "      <td>-1.026853</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.418600</td>\n",
       "      <td>-0.929668</td>\n",
       "      <td>1.284014</td>\n",
       "      <td>0.731842</td>\n",
       "      <td>0.801786</td>\n",
       "      <td>-0.728297</td>\n",
       "      <td>-0.412095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.760983</td>\n",
       "      <td>0.515132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.673905</td>\n",
       "      <td>-0.393862</td>\n",
       "      <td>-1.584207</td>\n",
       "      <td>-0.439778</td>\n",
       "      <td>0.796104</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.546275</td>\n",
       "      <td>-1.489542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.622007</td>\n",
       "      <td>-0.473156</td>\n",
       "      <td>0.780020</td>\n",
       "      <td>0.648577</td>\n",
       "      <td>0.646100</td>\n",
       "      <td>-0.789362</td>\n",
       "      <td>0.083349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.658855</td>\n",
       "      <td>0.915052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.581082</td>\n",
       "      <td>0.477199</td>\n",
       "      <td>-0.622226</td>\n",
       "      <td>0.390642</td>\n",
       "      <td>0.753299</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.485999</td>\n",
       "      <td>0.586012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.361481</td>\n",
       "      <td>-0.364566</td>\n",
       "      <td>-1.318596</td>\n",
       "      <td>-0.385155</td>\n",
       "      <td>0.140133</td>\n",
       "      <td>0.123245</td>\n",
       "      <td>-0.670030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.638854</td>\n",
       "      <td>0.314099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>1.102342</td>\n",
       "      <td>-0.807371</td>\n",
       "      <td>0.329158</td>\n",
       "      <td>0.484305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.321985</td>\n",
       "      <td>-0.075827</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.629672</td>\n",
       "      <td>0.876864</td>\n",
       "      <td>0.411271</td>\n",
       "      <td>0.433440</td>\n",
       "      <td>0.997364</td>\n",
       "      <td>2.829590</td>\n",
       "      <td>-1.275588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.091376</td>\n",
       "      <td>0.859811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.505439</td>\n",
       "      <td>1.665086</td>\n",
       "      <td>-0.912464</td>\n",
       "      <td>-0.332054</td>\n",
       "      <td>0.707705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.828886</td>\n",
       "      <td>0.140387</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.624304</td>\n",
       "      <td>-2.197691</td>\n",
       "      <td>-1.479267</td>\n",
       "      <td>-0.465917</td>\n",
       "      <td>-0.014757</td>\n",
       "      <td>-0.320434</td>\n",
       "      <td>-0.511896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20614</th>\n",
       "      <td>0</td>\n",
       "      <td>16740</td>\n",
       "      <td>1.054036</td>\n",
       "      <td>0.781726</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.354015</td>\n",
       "      <td>0.031608</td>\n",
       "      <td>0.505571</td>\n",
       "      <td>-0.391351</td>\n",
       "      <td>-0.362796</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.546639</td>\n",
       "      <td>-1.083213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.141812</td>\n",
       "      <td>1.298495</td>\n",
       "      <td>0.485861</td>\n",
       "      <td>-0.284795</td>\n",
       "      <td>0.445140</td>\n",
       "      <td>-1.649364</td>\n",
       "      <td>0.901836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20615</th>\n",
       "      <td>0</td>\n",
       "      <td>16741</td>\n",
       "      <td>-0.181712</td>\n",
       "      <td>-1.703955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125573</td>\n",
       "      <td>-1.236718</td>\n",
       "      <td>0.360849</td>\n",
       "      <td>-0.527227</td>\n",
       "      <td>-0.812237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.865396</td>\n",
       "      <td>0.073210</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.802118</td>\n",
       "      <td>-2.277912</td>\n",
       "      <td>-0.227848</td>\n",
       "      <td>-0.020259</td>\n",
       "      <td>-1.445771</td>\n",
       "      <td>1.290703</td>\n",
       "      <td>-0.015311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20616</th>\n",
       "      <td>0</td>\n",
       "      <td>16742</td>\n",
       "      <td>-0.322872</td>\n",
       "      <td>-0.339665</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.725504</td>\n",
       "      <td>-0.260038</td>\n",
       "      <td>0.651661</td>\n",
       "      <td>-0.116402</td>\n",
       "      <td>-0.154363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.601178</td>\n",
       "      <td>0.298002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.332212</td>\n",
       "      <td>0.994915</td>\n",
       "      <td>0.389641</td>\n",
       "      <td>0.073859</td>\n",
       "      <td>-1.399902</td>\n",
       "      <td>-0.097716</td>\n",
       "      <td>-1.579794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20617</th>\n",
       "      <td>0</td>\n",
       "      <td>16743</td>\n",
       "      <td>-0.044867</td>\n",
       "      <td>1.307402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.494602</td>\n",
       "      <td>-0.573069</td>\n",
       "      <td>0.401516</td>\n",
       "      <td>0.386289</td>\n",
       "      <td>0.155312</td>\n",
       "      <td>...</td>\n",
       "      <td>1.135809</td>\n",
       "      <td>-0.869840</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.099552</td>\n",
       "      <td>0.981087</td>\n",
       "      <td>0.457334</td>\n",
       "      <td>0.882776</td>\n",
       "      <td>-1.473003</td>\n",
       "      <td>1.360535</td>\n",
       "      <td>-0.680987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20618</th>\n",
       "      <td>0</td>\n",
       "      <td>16744</td>\n",
       "      <td>-0.627163</td>\n",
       "      <td>-0.659393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.244088</td>\n",
       "      <td>0.715901</td>\n",
       "      <td>1.338035</td>\n",
       "      <td>-0.353917</td>\n",
       "      <td>-1.968249</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.139956</td>\n",
       "      <td>-0.794772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.905882</td>\n",
       "      <td>0.496918</td>\n",
       "      <td>-0.100222</td>\n",
       "      <td>0.381162</td>\n",
       "      <td>1.267991</td>\n",
       "      <td>0.053304</td>\n",
       "      <td>0.536955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20619 rows × 420 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       target     id  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0           0      0   0.468142  -1.045346        0.0   0.384487   0.435121   \n",
       "1           0      1  -0.760983   0.515132        0.0  -1.673905  -0.393862   \n",
       "2           0      2   1.658855   0.915052        0.0  -0.581082   0.477199   \n",
       "3           0      3  -0.638854   0.314099        0.0   0.000919   1.102342   \n",
       "4           0      4  -1.091376   0.859811        0.0  -0.505439   1.665086   \n",
       "...       ...    ...        ...        ...        ...        ...        ...   \n",
       "20614       0  16740   1.054036   0.781726        0.0   0.354015   0.031608   \n",
       "20615       0  16741  -0.181712  -1.703955        0.0   0.125573  -1.236718   \n",
       "20616       0  16742  -0.322872  -0.339665        0.0  -0.725504  -0.260038   \n",
       "20617       0  16743  -0.044867   1.307402        0.0   0.494602  -0.573069   \n",
       "20618       0  16744  -0.627163  -0.659393        0.0   0.244088   0.715901   \n",
       "\n",
       "       feature_6  feature_7  feature_8  ...  feature_409  feature_410  \\\n",
       "0      -1.178548   0.124543   1.801544  ...    -0.361507    -1.026853   \n",
       "1      -1.584207  -0.439778   0.796104  ...    -0.546275    -1.489542   \n",
       "2      -0.622226   0.390642   0.753299  ...    -0.485999     0.586012   \n",
       "3      -0.807371   0.329158   0.484305  ...     0.321985    -0.075827   \n",
       "4      -0.912464  -0.332054   0.707705  ...     0.828886     0.140387   \n",
       "...          ...        ...        ...  ...          ...          ...   \n",
       "20614   0.505571  -0.391351  -0.362796  ...    -0.546639    -1.083213   \n",
       "20615   0.360849  -0.527227  -0.812237  ...     0.865396     0.073210   \n",
       "20616   0.651661  -0.116402  -0.154363  ...     0.601178     0.298002   \n",
       "20617   0.401516   0.386289   0.155312  ...     1.135809    -0.869840   \n",
       "20618   1.338035  -0.353917  -1.968249  ...    -2.139956    -0.794772   \n",
       "\n",
       "       feature_411  feature_412  feature_413  feature_414  feature_415  \\\n",
       "0              0.0     1.418600    -0.929668     1.284014     0.731842   \n",
       "1              0.0    -0.622007    -0.473156     0.780020     0.648577   \n",
       "2              0.0     0.361481    -0.364566    -1.318596    -0.385155   \n",
       "3              0.0    -1.629672     0.876864     0.411271     0.433440   \n",
       "4              0.0    -0.624304    -2.197691    -1.479267    -0.465917   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "20614          0.0    -0.141812     1.298495     0.485861    -0.284795   \n",
       "20615          0.0    -0.802118    -2.277912    -0.227848    -0.020259   \n",
       "20616          0.0     1.332212     0.994915     0.389641     0.073859   \n",
       "20617          0.0    -1.099552     0.981087     0.457334     0.882776   \n",
       "20618          0.0    -1.905882     0.496918    -0.100222     0.381162   \n",
       "\n",
       "       feature_416  feature_417  feature_418  \n",
       "0         0.801786    -0.728297    -0.412095  \n",
       "1         0.646100    -0.789362     0.083349  \n",
       "2         0.140133     0.123245    -0.670030  \n",
       "3         0.997364     2.829590    -1.275588  \n",
       "4        -0.014757    -0.320434    -0.511896  \n",
       "...            ...          ...          ...  \n",
       "20614     0.445140    -1.649364     0.901836  \n",
       "20615    -1.445771     1.290703    -0.015311  \n",
       "20616    -1.399902    -0.097716    -1.579794  \n",
       "20617    -1.473003     1.360535    -0.680987  \n",
       "20618     1.267991     0.053304     0.536955  \n",
       "\n",
       "[20619 rows x 420 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"merged_train.csv\")\n",
    "del df[\"smpl\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T12:56:10.403560Z",
     "start_time": "2024-11-07T12:56:09.168360Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop([\"target\"],axis=1)\n",
    "y = df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T12:56:10.410186Z",
     "start_time": "2024-11-07T12:56:09.264886Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T12:56:24.013719Z",
     "start_time": "2024-11-07T12:56:09.357612Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Объединяем X и y для обучающего и тестового наборов\n",
    "train_data = X_train.copy()\n",
    "train_data['label'] = y_train\n",
    "train_data['id'] = X_train['id']\n",
    "\n",
    "test_data = X_test.copy()\n",
    "test_data['label'] = y_test\n",
    "test_data['id'] = X_test['id']\n",
    "# Функция для создания последовательностей\n",
    "def create_sequences(data):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    ids = data['id'].unique()\n",
    "    for id_ in ids:\n",
    "        group = data[data['id'] == id_]\n",
    "        seq_features = group.drop(['label', 'id'], axis=1).values\n",
    "        seq_label = group['label'].values[-1]  # Предположим, что метка — последняя в последовательности\n",
    "        sequences.append(seq_features)\n",
    "        labels.append(seq_label)\n",
    "    return sequences, labels\n",
    "\n",
    "# Создаем последовательности для обучения и тестирования\n",
    "X_train_seq, y_train_seq = create_sequences(train_data)\n",
    "X_test_seq, y_test_seq = create_sequences(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T12:56:26.959901Z",
     "start_time": "2024-11-07T12:56:23.997771Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95)  # Сохраняем 95% дисперсии\n",
    "X_pca = pca.fit_transform(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T12:56:27.489799Z",
     "start_time": "2024-11-07T12:56:26.964664Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
    "plt.xlabel('Количество компонент')\n",
    "plt.ylabel('Накопленная объясненная дисперсия')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T12:56:27.493992Z",
     "start_time": "2024-11-07T12:56:27.476303Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_409</th>\n",
       "      <th>feature_410</th>\n",
       "      <th>feature_411</th>\n",
       "      <th>feature_412</th>\n",
       "      <th>feature_413</th>\n",
       "      <th>feature_414</th>\n",
       "      <th>feature_415</th>\n",
       "      <th>feature_416</th>\n",
       "      <th>feature_417</th>\n",
       "      <th>feature_418</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.012667</td>\n",
       "      <td>0.006476</td>\n",
       "      <td>0.005283</td>\n",
       "      <td>-0.110996</td>\n",
       "      <td>0.109232</td>\n",
       "      <td>0.043099</td>\n",
       "      <td>0.086832</td>\n",
       "      <td>0.084686</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>0.009610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007505</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>-0.014867</td>\n",
       "      <td>-0.002319</td>\n",
       "      <td>0.014614</td>\n",
       "      <td>0.126005</td>\n",
       "      <td>0.117004</td>\n",
       "      <td>0.043388</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.007698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.143411</td>\n",
       "      <td>0.003812</td>\n",
       "      <td>0.006763</td>\n",
       "      <td>0.092776</td>\n",
       "      <td>-0.037888</td>\n",
       "      <td>-0.039911</td>\n",
       "      <td>0.133609</td>\n",
       "      <td>-0.092462</td>\n",
       "      <td>-0.056059</td>\n",
       "      <td>0.015424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008155</td>\n",
       "      <td>-0.001404</td>\n",
       "      <td>0.021682</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>0.026661</td>\n",
       "      <td>-0.050341</td>\n",
       "      <td>-0.051373</td>\n",
       "      <td>-0.021668</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>-0.000568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.071995</td>\n",
       "      <td>-0.023934</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.115675</td>\n",
       "      <td>0.072783</td>\n",
       "      <td>-0.048022</td>\n",
       "      <td>-0.038713</td>\n",
       "      <td>-0.125888</td>\n",
       "      <td>0.006077</td>\n",
       "      <td>-0.006916</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016931</td>\n",
       "      <td>-0.000662</td>\n",
       "      <td>0.122816</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>-0.008769</td>\n",
       "      <td>0.062708</td>\n",
       "      <td>0.079164</td>\n",
       "      <td>0.025406</td>\n",
       "      <td>0.001945</td>\n",
       "      <td>-0.008388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.034301</td>\n",
       "      <td>0.016065</td>\n",
       "      <td>-0.000669</td>\n",
       "      <td>-0.087006</td>\n",
       "      <td>-0.033124</td>\n",
       "      <td>0.037362</td>\n",
       "      <td>0.018790</td>\n",
       "      <td>0.116706</td>\n",
       "      <td>-0.000794</td>\n",
       "      <td>0.003280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010813</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.188160</td>\n",
       "      <td>-0.000970</td>\n",
       "      <td>0.006403</td>\n",
       "      <td>-0.022682</td>\n",
       "      <td>-0.037718</td>\n",
       "      <td>-0.009946</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.005233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.154789</td>\n",
       "      <td>-0.002881</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>-0.001088</td>\n",
       "      <td>-0.006894</td>\n",
       "      <td>0.003144</td>\n",
       "      <td>-0.017701</td>\n",
       "      <td>-0.014938</td>\n",
       "      <td>-0.019262</td>\n",
       "      <td>0.020234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007910</td>\n",
       "      <td>0.002625</td>\n",
       "      <td>0.005165</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.056073</td>\n",
       "      <td>-0.003948</td>\n",
       "      <td>0.013154</td>\n",
       "      <td>0.007665</td>\n",
       "      <td>-0.000898</td>\n",
       "      <td>0.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>-0.010412</td>\n",
       "      <td>-0.005892</td>\n",
       "      <td>0.006466</td>\n",
       "      <td>-0.041117</td>\n",
       "      <td>0.058096</td>\n",
       "      <td>-0.014977</td>\n",
       "      <td>-0.017864</td>\n",
       "      <td>0.049927</td>\n",
       "      <td>-0.000792</td>\n",
       "      <td>-0.007290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028538</td>\n",
       "      <td>-0.000261</td>\n",
       "      <td>0.153388</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>-0.002005</td>\n",
       "      <td>0.012148</td>\n",
       "      <td>0.015417</td>\n",
       "      <td>0.009280</td>\n",
       "      <td>-0.007403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>0.022149</td>\n",
       "      <td>0.003794</td>\n",
       "      <td>-0.001803</td>\n",
       "      <td>0.028692</td>\n",
       "      <td>-0.014652</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>0.051042</td>\n",
       "      <td>-0.017959</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014928</td>\n",
       "      <td>0.015503</td>\n",
       "      <td>0.041421</td>\n",
       "      <td>-0.010954</td>\n",
       "      <td>0.014814</td>\n",
       "      <td>0.007834</td>\n",
       "      <td>0.069368</td>\n",
       "      <td>-0.012776</td>\n",
       "      <td>-0.005399</td>\n",
       "      <td>0.012128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>0.017408</td>\n",
       "      <td>0.006947</td>\n",
       "      <td>-0.023253</td>\n",
       "      <td>0.037552</td>\n",
       "      <td>0.023956</td>\n",
       "      <td>0.013277</td>\n",
       "      <td>-0.017212</td>\n",
       "      <td>-0.045593</td>\n",
       "      <td>-0.001261</td>\n",
       "      <td>-0.009279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009716</td>\n",
       "      <td>0.002960</td>\n",
       "      <td>-0.070092</td>\n",
       "      <td>-0.016140</td>\n",
       "      <td>-0.009435</td>\n",
       "      <td>-0.007155</td>\n",
       "      <td>0.115719</td>\n",
       "      <td>0.050600</td>\n",
       "      <td>-0.007503</td>\n",
       "      <td>0.000820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0.026855</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>0.012535</td>\n",
       "      <td>0.017034</td>\n",
       "      <td>-0.104502</td>\n",
       "      <td>-0.011821</td>\n",
       "      <td>0.021558</td>\n",
       "      <td>0.043395</td>\n",
       "      <td>0.003667</td>\n",
       "      <td>0.013806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005560</td>\n",
       "      <td>0.001555</td>\n",
       "      <td>0.102758</td>\n",
       "      <td>-0.010485</td>\n",
       "      <td>-0.014218</td>\n",
       "      <td>-0.031690</td>\n",
       "      <td>0.136519</td>\n",
       "      <td>0.006921</td>\n",
       "      <td>0.008335</td>\n",
       "      <td>0.004775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.018054</td>\n",
       "      <td>-0.002184</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>0.019101</td>\n",
       "      <td>0.054588</td>\n",
       "      <td>-0.003927</td>\n",
       "      <td>-0.030850</td>\n",
       "      <td>-0.010445</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.006630</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012683</td>\n",
       "      <td>-0.005287</td>\n",
       "      <td>0.153736</td>\n",
       "      <td>-0.005717</td>\n",
       "      <td>0.007250</td>\n",
       "      <td>-0.001683</td>\n",
       "      <td>-0.100139</td>\n",
       "      <td>-0.041052</td>\n",
       "      <td>0.011704</td>\n",
       "      <td>0.001734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>337 rows × 419 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0    0.012667   0.006476   0.005283  -0.110996   0.109232   0.043099   \n",
       "1    0.143411   0.003812   0.006763   0.092776  -0.037888  -0.039911   \n",
       "2   -0.071995  -0.023934   0.000703   0.115675   0.072783  -0.048022   \n",
       "3    0.034301   0.016065  -0.000669  -0.087006  -0.033124   0.037362   \n",
       "4    0.154789  -0.002881   0.019900  -0.001088  -0.006894   0.003144   \n",
       "..        ...        ...        ...        ...        ...        ...   \n",
       "332 -0.010412  -0.005892   0.006466  -0.041117   0.058096  -0.014977   \n",
       "333  0.022149   0.003794  -0.001803   0.028692  -0.014652   0.025900   \n",
       "334  0.017408   0.006947  -0.023253   0.037552   0.023956   0.013277   \n",
       "335  0.026855  -0.000054   0.012535   0.017034  -0.104502  -0.011821   \n",
       "336  0.018054  -0.002184   0.003179   0.019101   0.054588  -0.003927   \n",
       "\n",
       "     feature_6  feature_7  feature_8  feature_9  ...  feature_409  \\\n",
       "0     0.086832   0.084686   0.004070   0.009610  ...     0.007505   \n",
       "1     0.133609  -0.092462  -0.056059   0.015424  ...     0.008155   \n",
       "2    -0.038713  -0.125888   0.006077  -0.006916  ...    -0.016931   \n",
       "3     0.018790   0.116706  -0.000794   0.003280  ...     0.010813   \n",
       "4    -0.017701  -0.014938  -0.019262   0.020234  ...     0.007910   \n",
       "..         ...        ...        ...        ...  ...          ...   \n",
       "332  -0.017864   0.049927  -0.000792  -0.007290  ...    -0.028538   \n",
       "333   0.051042  -0.017959   0.003700   0.000881  ...     0.014928   \n",
       "334  -0.017212  -0.045593  -0.001261  -0.009279  ...     0.009716   \n",
       "335   0.021558   0.043395   0.003667   0.013806  ...     0.005560   \n",
       "336  -0.030850  -0.010445   0.000033  -0.006630  ...    -0.012683   \n",
       "\n",
       "     feature_410  feature_411  feature_412  feature_413  feature_414  \\\n",
       "0       0.001564    -0.014867    -0.002319     0.014614     0.126005   \n",
       "1      -0.001404     0.021682     0.001675     0.026661    -0.050341   \n",
       "2      -0.000662     0.122816     0.001497    -0.008769     0.062708   \n",
       "3       0.000062     0.188160    -0.000970     0.006403    -0.022682   \n",
       "4       0.002625     0.005165     0.000620     0.056073    -0.003948   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "332    -0.000261     0.153388     0.002283     0.002829    -0.002005   \n",
       "333     0.015503     0.041421    -0.010954     0.014814     0.007834   \n",
       "334     0.002960    -0.070092    -0.016140    -0.009435    -0.007155   \n",
       "335     0.001555     0.102758    -0.010485    -0.014218    -0.031690   \n",
       "336    -0.005287     0.153736    -0.005717     0.007250    -0.001683   \n",
       "\n",
       "     feature_415  feature_416  feature_417  feature_418  \n",
       "0       0.117004     0.043388     0.000369     0.007698  \n",
       "1      -0.051373    -0.021668     0.000620    -0.000568  \n",
       "2       0.079164     0.025406     0.001945    -0.008388  \n",
       "3      -0.037718    -0.009946     0.002256     0.005233  \n",
       "4       0.013154     0.007665    -0.000898     0.014200  \n",
       "..           ...          ...          ...          ...  \n",
       "332     0.012148     0.015417     0.009280    -0.007403  \n",
       "333     0.069368    -0.012776    -0.005399     0.012128  \n",
       "334     0.115719     0.050600    -0.007503     0.000820  \n",
       "335     0.136519     0.006921     0.008335     0.004775  \n",
       "336    -0.100139    -0.041052     0.011704     0.001734  \n",
       "\n",
       "[337 rows x 419 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "components = pd.DataFrame(pca.components_, columns=X.columns)\n",
    "display(components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T07:36:55.694037Z",
     "start_time": "2024-11-05T07:36:55.645172Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используется устройство: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Устанавливаем устройство (CPU или GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Используется устройство: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T07:36:55.695186Z",
     "start_time": "2024-11-05T07:36:55.682471Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        # Преобразуем в тензоры\n",
    "        sequence = torch.tensor(sequence, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return sequence, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T07:36:55.704419Z",
     "start_time": "2024-11-05T07:36:55.684853Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Создаем экземпляры Dataset\n",
    "train_dataset = SequenceDataset(X_train_seq, y_train_seq)\n",
    "test_dataset = SequenceDataset(X_test_seq, y_test_seq)\n",
    "\n",
    "# Создаем DataLoader\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T07:36:55.705050Z",
     "start_time": "2024-11-05T07:36:55.701153Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences = [item[0] for item in batch]\n",
    "    labels = torch.tensor([item[1] for item in batch], dtype=torch.float32)\n",
    "\n",
    "    # Сортируем последовательности по длине (требуется для pack_padded_sequence)\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "    lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "    sequences = [sequences[i] for i in perm_idx]\n",
    "    labels = labels[perm_idx]\n",
    "\n",
    "    # Паддируем последовательности\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True)\n",
    "\n",
    "    return sequences_padded, lengths, labels\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T07:36:55.938716Z",
     "start_time": "2024-11-05T07:36:55.709510Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Слой LSTM\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=2, batch_first=True)\n",
    "        # Полносвязный слой\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        # Сигмоидная функция активации\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Пакуем последовательности\n",
    "        packed_input = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        # Передаем через LSTM\n",
    "        packed_output, (h_n, c_n) = self.lstm(packed_input)\n",
    "        # Используем последний скрытый слой\n",
    "        h_n = h_n[-1]  # shape: (batch_size, hidden_size)\n",
    "        # Проходим через полносвязный слой и сигмоид\n",
    "        out = self.fc(h_n)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T07:36:55.986790Z",
     "start_time": "2024-11-05T07:36:55.720478Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (lstm): LSTM(418, 64, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Определяем размеры\n",
    "input_size = X_train_seq[0].shape[1]  # Количество признаков\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "\n",
    "# Инициализируем модель\n",
    "model = RNNModel(input_size, hidden_size, num_layers)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T07:37:40.145227Z",
     "start_time": "2024-11-05T07:36:55.755827Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 0.3152\n",
      "Epoch [2/30], Loss: 0.0960\n",
      "Epoch [3/30], Loss: 0.0919\n",
      "Epoch [4/30], Loss: 0.0892\n",
      "Epoch [5/30], Loss: 0.0864\n",
      "Epoch [6/30], Loss: 0.0832\n",
      "Epoch [7/30], Loss: 0.0795\n",
      "Epoch [8/30], Loss: 0.0756\n",
      "Epoch [9/30], Loss: 0.0705\n",
      "Epoch [10/30], Loss: 0.0650\n",
      "Epoch [11/30], Loss: 0.0590\n",
      "Epoch [12/30], Loss: 0.0521\n",
      "Epoch [13/30], Loss: 0.0452\n",
      "Epoch [14/30], Loss: 0.0378\n",
      "Epoch [15/30], Loss: 0.0305\n",
      "Epoch [16/30], Loss: 0.0237\n",
      "Epoch [17/30], Loss: 0.0178\n",
      "Epoch [18/30], Loss: 0.0128\n",
      "Epoch [19/30], Loss: 0.0089\n",
      "Epoch [20/30], Loss: 0.0061\n",
      "Epoch [21/30], Loss: 0.0041\n",
      "Epoch [22/30], Loss: 0.0026\n",
      "Epoch [23/30], Loss: 0.0016\n",
      "Epoch [24/30], Loss: 0.0010\n",
      "Epoch [25/30], Loss: 0.0007\n",
      "Epoch [26/30], Loss: 0.0005\n",
      "Epoch [27/30], Loss: 0.0003\n",
      "Epoch [28/30], Loss: 0.0002\n",
      "Epoch [29/30], Loss: 0.0002\n",
      "Epoch [30/30], Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "num_epochs = 30\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for sequences_padded, lengths, labels in train_loader:\n",
    "        sequences_padded = sequences_padded.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        # Обнуляем градиенты\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Прямой проход\n",
    "        outputs = model(sequences_padded, lengths)\n",
    "\n",
    "        # Вычисляем потерю\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #cross_loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Обратный проход и оптимизация\n",
    "        loss.backward()\n",
    "        #cross_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:00:56.321522Z",
     "start_time": "2024-11-05T08:00:54.934446Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_pred_proba = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequences_padded, lengths, labels in test_loader:\n",
    "        sequences_padded = sequences_padded.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        outputs = model(sequences_padded, lengths)\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred_proba.extend(outputs.cpu().numpy())\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:00:58.518324Z",
     "start_time": "2024-11-05T08:00:58.280545Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.011904761904761904\n",
      "Precision: 0.1\n",
      "ROC-AUC: 0.620170320603489\n",
      "Confusion Matrix:\n",
      "[[4031    9]\n",
      " [  83    1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "y_true = np.array(y_true).flatten()\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "y_pred_proba = np.array(y_pred_proba).flatten()\n",
    "\n",
    "recall_rnn = recall_score(y_true, y_pred)\n",
    "precision_rnn = precision_score(y_true, y_pred)\n",
    "roc_auc_rnn = roc_auc_score(y_true, y_pred_proba)\n",
    "conf_matrix_rnn = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(f\"Recall: {recall_rnn}\")\n",
    "print(f\"Precision: {precision_rnn}\")\n",
    "print(f\"ROC-AUC: {roc_auc_rnn}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix_rnn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-05T07:36:14.811505Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import recall_score, precision_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Предположим, что X_train_seq, y_train_seq, X_test_seq, y_test_seq уже созданы\n",
    "\n",
    "# Масштабирование данных\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = [scaler.fit_transform(seq) for seq in X_train_seq]\n",
    "X_test_scaled = [scaler.transform(seq) for seq in X_test_seq]\n",
    "\n",
    "# Определение Dataset и DataLoader\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        sequence = torch.tensor(sequence, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return sequence, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long)\n",
    "    sequences_padded = nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "    sequences_padded = sequences_padded[perm_idx]\n",
    "    labels = labels[perm_idx]\n",
    "    return sequences_padded, lengths, labels\n",
    "\n",
    "train_dataset = SequenceDataset(X_train_scaled, y_train_seq)\n",
    "test_dataset = SequenceDataset(X_test_scaled, y_test_seq)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Определение Модели с Bidirectional LSTM и Dropout\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, bidirectional=True, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                            batch_first=True, bidirectional=bidirectional, dropout=dropout)\n",
    "        direction_factor = 2 if bidirectional else 1\n",
    "        self.fc = nn.Linear(hidden_size * direction_factor, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True)\n",
    "        packed_output, (h_n, c_n) = self.lstm(packed_input)\n",
    "        if self.bidirectional:\n",
    "            # Конкатенация скрытых состояний из обоих направлений\n",
    "            h_n = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        else:\n",
    "            h_n = h_n[-1]\n",
    "        out = self.fc(h_n)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# Инициализация Модели\n",
    "input_size = X_train_scaled[0].shape[1]\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "\n",
    "model = RNNModel(input_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "# Взвешенные Потери\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_seq), y=y_train_seq)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "criterion = nn.BCELoss(weight=class_weights[y_train_seq].to(device))  # Убедитесь, что веса корректны\n",
    "\n",
    "# Оптимизатор с L2 Регуляризацией\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Обучение Модели\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for sequences_padded, lengths, labels in train_loader:\n",
    "        sequences_padded = sequences_padded.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences_padded, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Оценка Модели\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_pred_proba = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequences_padded, lengths, labels in test_loader:\n",
    "        sequences_padded = sequences_padded.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        outputs = model(sequences_padded, lengths)\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred_proba.extend(outputs.cpu().numpy())\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "y_true = np.array(y_true).flatten()\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "y_pred_proba = np.array(y_pred_proba).flatten()\n",
    "\n",
    "recall_rnn = recall_score(y_true, y_pred)\n",
    "precision_rnn = precision_score(y_true, y_pred)\n",
    "roc_auc_rnn = roc_auc_score(y_true, y_pred_proba)\n",
    "conf_matrix_rnn = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(f\"Recall: {recall_rnn}\")\n",
    "print(f\"Precision: {precision_rnn}\")\n",
    "print(f\"ROC-AUC: {roc_auc_rnn}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix_rnn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-05T07:36:14.811664Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(\"Train class distribution:\", Counter(y_train_seq))\n",
    "print(\"Test class distribution:\", Counter(y_test_seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
